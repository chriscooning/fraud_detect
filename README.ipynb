{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud: Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jessica Mouras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[1. Motivation](#motive)<br> \n",
    "[2. Data](#data)\n",
    ">   [i. Pipeline](#pipeline)<br>\n",
    "    [ii. Intro to Fraud](#intro)<br>\n",
    "    [iii. Unbalanced Datasets](#unbalanced)<br>\n",
    "    \n",
    "[3. Machine Learning Classification Models](#mlclass)\n",
    "> [i. Undersampling](#undersample)<br>\n",
    "> [ii. SMOTE](#smote)<br>\n",
    "> [iii. Isolation Forest](#isolation)<br>\n",
    "> [iv. PCA Anomaly Detection](#pca)<br>\n",
    "> [v. Neural Network Autoencoder](#auto)<br>\n",
    "> [vi. Best Model](#best)<br>\n",
    "\n",
    "[4. Conclusion & Further Application](#conclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "<a id=\"motive\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started my career in financial services As a CPA (license still current, but inactive) who found the gritter aspects of accounting and finance most intriguing. In fact, my first consulting project ever was an Anti Money Laundering (AML) assignment that deployed Machine Learning techniques, but was 2012 and I was a very junior employee. \n",
    "\n",
    "I wanted to revisit the topic of money laundering by building methods and models to assist with identifying fraudulent transactions.\n",
    "\n",
    "Here, I assess a large dataset of credit card transactions over 2 days in September 2013 of European cardholders to classify the transactions between fraud and non-fraud. As discussed below, by doing this classification, I am detecting anomalies. Through my analysis, I hope to determine which methods are best for detecting fraudlent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "<a id=\"data\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed briefly above, this dataset contains transactions made by European credit holders cards in September 2013 over the coure of two days. The minority class is  492 frauds out of 284,807 transactions which is 0.172% of all transactions. Therefore, the majority class is 99.827% of transactions.\n",
    "\n",
    "The following had already been done to the data to anonymize the information except for time and amount:\n",
    "\n",
    "+ PCA Transformation:  the features went through a PCA transformation (Dimensionality Reduction technique) which creates latent features that are some combination of the original features of the data. \n",
    "\n",
    "+ Scaling: The data has already been scaled as in order to perform PCA, one must scale the features prior to transformation.\n",
    "\n",
    "I separately scaled time and amount to perform the remainder of this analysis.\n",
    "\n",
    "Due to the anonymization process, the feature names, descriptions, and nature is largely undisclosed. I reviewed the features for data-leakage and potential rank issues (where a feature is a transformation of another feature and therefore redundant). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "<a id=\"pipeline\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform my analysis I used the following libaries:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"660\" height=\"300\" src=\"images/keras_regression_logos.png\">\n",
    "</p>\n",
    "\n",
    "\n",
    "The data was already classified as 1 - the class for fraudulent transaction and 0 - the class for normal transactions. Throughout this analysis I will refer to predicting a fraud instance correctly to be considered a true positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Fraud\n",
    "<a id=\"intro\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of financial fraud and many subsets of Money Laundering. In the course of this notebook, we will only be addressing the concept of Money Laundering through a Retail Banking Institution. Also known as: Credit Card Fraud.\n",
    "\n",
    ">*Credit card fraud is the unauthorized use of a credit or debit card, or similar payment tool (ACH, EFT, recurring charge, etc.), to fraudulently obtain money or property.* - **Federal Bureau of Investigation**\n",
    "\n",
    "Disclaimer: the methods deployed in this notebook may not be applicable or appropriate for other types of Money Laundering e.g. through an Investment Bank / Private Equity Fund, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced Datasets\n",
    "<a id=\"unbalanced\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced data in terms of a classification models means that there is proportionally more of one class (taget) than the other. This is an issue because what happens during training and deployment of machine learning classifiers is  that there are not enough examples of the minority class for a model to effectively learn the \"decision boundary\".\n",
    "\n",
    "All models will struggle on new or true testing data as they just didn't have enough evidence and \"coaching\" to learn the difference!\n",
    "\n",
    "There are two ways to attempt to solve this issue:\n",
    "\n",
    "1. Undersampling\n",
    "2. Oversampling\n",
    "3. Other Techniques: Anomaly Detection\n",
    "\n",
    "**Undersampling**\n",
    "\n",
    "We find out how many instances are in the minority class,  fraudulent transactions in this case.\n",
    "Then we need to make a new data set that is a randomized (shuffled) subsample of the majority class, normal transactions, to the same amount as fraud transactions. This creates an even, balanced data set of 50% of each class (for binary classification such as fraud vs not fraud. For this dataset that means we limit our original dataset to be 492 cases of fraud and 492 cases of normal transactions.\n",
    "\n",
    "Warning: This methodology of undersampling comes at a relatively large price. The original data set was approximately 284,000 transactions, and now it has been reduced to 984.There is a risk that a classification model will not perform well since there is large volumes of general information loss.\n",
    "\n",
    "One way to solve this information loss issue brings us to our other option:\n",
    "\n",
    "**Oversampling**\n",
    "\n",
    "You want to oversample instances the minority class. How do we oversample something that doesn't exist? \n",
    "\n",
    "**a.** Make copies of exact samples from the minority class in the training dataset prior to fitting a model. Rather simplistic and doesn't actually assist in information gain like actual new data would provide.\n",
    "\n",
    "**b.** Synthesize new instances from the minority class. This methodology is called Synthetic Minority Oversampling TEchnique, or SMOTE for short. This technique was described by Nitesh Chawla, et al. in their 2002 paper named for the technique titled “SMOTE: Synthetic Minority Over-sampling Technique.”\n",
    "\n",
    "**Anomaly Detection Techniques**\n",
    "\n",
    "*Isolation Forest*: isolate anomaly values through a supervised or unsupervised classifier. Anomalies are the ponts with the shortest average path length.\n",
    "\n",
    "*Auto-encoders*: detect anomaly values through an unsupervised neural network in which we measure how “far” the reconstructed data point provived by the model is from the actual, original datapoint. If the error is large, then the original datapoint is likely an anomaly. In this analysis we perform 2 types of this technique, one based on linear compression and the other with nonlinear compression.\n",
    "\n",
    "As such, during the EDA process, I did not choose to select outliers to remove, as they could be possible important transactions to review during the anomaly classification process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Classification Models\n",
    "<a id=\"mlclass\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models analyzed over the course of this analysis are as follows:\n",
    "\n",
    "Undersampling:\n",
    "+ Logistic Regression\n",
    "+ kNN Classifier\n",
    "+ Decision Trees Classifier\n",
    "+ Random Forest Classifier\n",
    "+ Gradient Boosted Classifier\n",
    "\n",
    "Oversampling (SMOTE):\n",
    "+ None\n",
    "\n",
    "Anomaly Detection on Unbalanced Classes:\n",
    "+ Isolation Forest\n",
    "+ PCA Anomaly Detection\n",
    "+ Neural Network Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling\n",
    "<a id=\"undersample\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "<a id=\"smote\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This oversampling technique ultimately selects instances that are close to each other in the existing dataset's feature space. It then \"draws\" a line between the instances selected and adds new sample at a point somwhere along that line.\n",
    "\n",
    "To put this into a perspective we can digest using machine learning methods and terms:\n",
    "\n",
    "Step 1) a random instance from the minority class is first chosen.\n",
    "Step 2) k of the nearest neighbors for that example are found (typically k=5).\n",
    "Step 3) a randomly selected \"neighbor\" is chosen.\n",
    "Step 4) finally, a synthetic example is created at a randomly selected point between the two examples in feature space.\n",
    "\n",
    "That is a lot of randoms!\n",
    "\n",
    "Why does this work? Well, \"new synthetic\" instances from the minority class that are generated through this process  are generally speaking close in feature space to existing examples from the minority class.\n",
    "\n",
    "This is not a very good idea for data with many features. The larger the dimension space, the more complex it is for kNN to collect nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain what is isolation forest \n",
    "explain model's strength regarding how gridsearching parameters is best performed on the contamination score.\n",
    "explain why you want to have a contamination score higher generally than your actual % of anomaly\n",
    "\n",
    "explain why you want high recall\n",
    "\n",
    "Show confusion matrix for best performing grid searched parameters here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss that this is linear decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ describe and discuss autoencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "<a id=\"best\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [placeholder for best model confusion matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [placeholder for conclusion]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
